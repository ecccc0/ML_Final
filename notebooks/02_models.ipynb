{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ae45a9",
   "metadata": {},
   "source": [
    "# Supervised Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a22f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34eaeef",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a548aa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Random State: 42\n",
      "  Target Column: SalePrice\n",
      "  Skewed Features to Transform: 18\n",
      "  Columns to Drop: ['Id']\n"
     ]
    }
   ],
   "source": [
    "# Configuration and constants\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COL = \"SalePrice\"\n",
    "\n",
    "# Skewed numeric features to log-transform (from EDA analysis with skewness > 0.75)\n",
    "SKEWED_FEATURES = [\n",
    "    'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "    'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n",
    "    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch',\n",
    "    '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'\n",
    "]\n",
    "\n",
    "# Columns to drop (Id is not useful for prediction)\n",
    "DROP_COLS = ['Id']\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Random State: {RANDOM_STATE}\")\n",
    "print(f\"  Target Column: {TARGET_COL}\")\n",
    "print(f\"  Skewed Features to Transform: {len(SKEWED_FEATURES)}\")\n",
    "print(f\"  Columns to Drop: {DROP_COLS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e8ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded:\n",
      "  Shape: (1460, 81)\n",
      "  Samples: 1460\n",
      "  Features: 81\n",
      "\n",
      "Dropped columns: ['Id']\n",
      "  New shape: (1460, 80)\n",
      "\n",
      "Missing values summary:\n",
      "  Total missing values: 7829\n",
      "  Columns with missing values: 19\n"
     ]
    }
   ],
   "source": [
    "# Load training data (same as in EDA)\n",
    "train_path = \"../data/train.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "print(f\"Training data loaded:\")\n",
    "print(f\"  Shape: {train_df.shape}\")\n",
    "print(f\"  Samples: {train_df.shape[0]}\")\n",
    "print(f\"  Features: {train_df.shape[1]}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "if any(col in train_df.columns for col in DROP_COLS):\n",
    "    train_df = train_df.drop(columns=DROP_COLS, errors='ignore')\n",
    "    print(f\"\\nDropped columns: {DROP_COLS}\")\n",
    "    print(f\"  New shape: {train_df.shape}\")\n",
    "\n",
    "# Quick check for missing values\n",
    "print(f\"\\nMissing values summary:\")\n",
    "missing_total = train_df.isna().sum().sum()\n",
    "print(f\"  Total missing values: {missing_total}\")\n",
    "print(f\"  Columns with missing values: {(train_df.isna().sum() > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf3a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable (y):\n",
      "  Name: log1p(SalePrice)\n",
      "  Shape: (1460,)\n",
      "  Min: 10.4603, Max: 13.5345, Mean: 12.0241\n",
      "\n",
      "Feature matrix (X):\n",
      "  Shape: (1460, 79)\n",
      "  Features: 79\n"
     ]
    }
   ],
   "source": [
    "# Define target and features\n",
    "# Apply log transformation to target (decision from EDA)\n",
    "y = np.log1p(train_df[TARGET_COL])\n",
    "\n",
    "# Features: all columns except target\n",
    "X = train_df.drop(columns=[TARGET_COL])\n",
    "\n",
    "print(f\"Target variable (y):\")\n",
    "print(f\"  Name: log1p({TARGET_COL})\")\n",
    "print(f\"  Shape: {y.shape}\")\n",
    "print(f\"  Min: {y.min():.4f}, Max: {y.max():.4f}, Mean: {y.mean():.4f}\")\n",
    "\n",
    "print(f\"\\nFeature matrix (X):\")\n",
    "print(f\"  Shape: {X.shape}\")\n",
    "print(f\"  Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde1506a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature type breakdown:\n",
      "  Numeric features: 36\n",
      "  Categorical features: 43\n",
      "  Total: 79\n",
      "\n",
      "Numeric features (36):\n",
      "['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n",
      "\n",
      "Categorical features (43):\n",
      "['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n"
     ]
    }
   ],
   "source": [
    "# Split numeric vs categorical features (same as in EDA)\n",
    "numeric_features = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "print(f\"Feature type breakdown:\")\n",
    "print(f\"  Numeric features: {len(numeric_features)}\")\n",
    "print(f\"  Categorical features: {len(categorical_features)}\")\n",
    "print(f\"  Total: {len(numeric_features) + len(categorical_features)}\")\n",
    "\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}):\")\n",
    "print(numeric_features)\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72535d0",
   "metadata": {},
   "source": [
    "## Preprocessing Strategy and Feature Space Definitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48df2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing strategy defined:\n",
      "  Missing values: Median (numeric), Most Frequent (categorical)\n",
      "  Scaling: StandardScaler for all numeric features\n",
      "  Target: log1p(SalePrice) for training, expm1 for predictions\n",
      "  Feature spaces: Z0 (baseline), Z1 (log), Z2 (polynomial), Z3 (PCA)\n",
      "  Outliers: Keep all data, rely on model robustness\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing strategy defined:\")\n",
    "print(\"  Missing values: Median (numeric), Most Frequent (categorical)\")\n",
    "print(\"  Scaling: StandardScaler for all numeric features\")\n",
    "print(\"  Target: log1p(SalePrice) for training, expm1 for predictions\")\n",
    "print(\"  Feature spaces: Z0 (baseline), Z1 (log), Z2 (polynomial), Z3 (PCA)\")\n",
    "print(\"  Outliers: Keep all data, rely on model robustness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15eb4652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before outlier filtering:\n",
      "  X shape: (1460, 79)\n",
      "  y shape: (1460,)\n",
      "\n",
      "Potential outliers identified (GrLivArea > 4000): 4\n",
      "  These 4 observations will be KEPT in the training set.\n",
      "  Rationale: Insufficient evidence of data errors, models are robust to outliers.\n",
      "\n",
      "After outlier filtering:\n",
      "  X shape: (1460, 79) (no change)\n",
      "  y shape: (1460,) (no change)\n"
     ]
    }
   ],
   "source": [
    "# Outlier handling: Implement decision to keep all data\n",
    "# Based on EDA, we identified some houses with large GrLivArea (>4000 sq ft)\n",
    "# Decision: Keep all observations, rely on model robustness and regularization\n",
    "\n",
    "print(f\"Before outlier filtering:\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n",
    "\n",
    "# Optionally, identify potential outliers for documentation\n",
    "outlier_threshold_grliv = 4000\n",
    "potential_outliers = X[X['GrLivArea'] > outlier_threshold_grliv]\n",
    "print(f\"\\nPotential outliers identified (GrLivArea > {outlier_threshold_grliv}): {len(potential_outliers)}\")\n",
    "\n",
    "if len(potential_outliers) > 0:\n",
    "    print(f\"  These {len(potential_outliers)} observations will be KEPT in the training set.\")\n",
    "    print(f\"  Rationale: Insufficient evidence of data errors, models are robust to outliers.\")\n",
    "\n",
    "# No filtering applied - all data retained\n",
    "print(f\"\\nAfter outlier filtering:\")\n",
    "print(f\"  X shape: {X.shape} (no change)\")\n",
    "print(f\"  y shape: {y.shape} (no change)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff7914",
   "metadata": {},
   "source": [
    "### Build Preprocessing Pipelines\n",
    "\n",
    "Now we implement the preprocessing pipelines that will be used across all feature spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bee344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base preprocessing pipelines created:\n",
      "  Numeric: Median imputation → Standard scaling\n",
      "  Categorical: Most frequent imputation → One-hot encoding\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# NUMERIC PREPROCESSING PIPELINE\n",
    "# Strategy: Median imputation + Standardization (mean=0, std=1)\n",
    "# Rationale: Median is robust to outliers; StandardScaler needed for \n",
    "#            distance-based and gradient-based models\n",
    "# ------------------------------------------------------------------------------\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CATEGORICAL PREPROCESSING PIPELINE  \n",
    "# Strategy: Most frequent imputation + One-hot encoding\n",
    "# Rationale: Missing often means \"None\" (e.g., no garage, no basement);\n",
    "#            most_frequent captures this well. handle_unknown='ignore' for\n",
    "#            robustness to unseen categories in test set.\n",
    "# ------------------------------------------------------------------------------\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "print(\"Base preprocessing pipelines created:\")\n",
    "print(\"  Numeric: Median imputation → Standard scaling\")\n",
    "print(\"  Categorical: Most frequent imputation → One-hot encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca161e",
   "metadata": {},
   "source": [
    "### Define Feature Spaces (Z0, Z1, Z2, Z3)\n",
    "\n",
    "We create preprocessors for each feature space that will be used for model training and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79817c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z0 (Baseline) preprocessor created:\n",
      "  Numeric features: 36\n",
      "  Categorical features: 43\n",
      "  Total input features: 79\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Z0: BASELINE FEATURE SPACE\n",
    "# Description: Standard preprocessing without feature engineering\n",
    "# Components: Standardized numerics + one-hot categoricals\n",
    "# ==============================================================================\n",
    "preprocessor_z0 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Z0 (Baseline) preprocessor created:\")\n",
    "print(f\"  Numeric features: {len(numeric_features)}\")\n",
    "print(f\"  Categorical features: {len(categorical_features)}\")\n",
    "print(f\"  Total input features: {len(numeric_features) + len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82946f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Log-transformed) feature space created:\n",
      "  Skewed features transformed: 18\n",
      "  Features: ['LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF']... (showing first 5)\n",
      "  Preprocessor: Same as Z0 (median impute + scale)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Z1: LOG-TRANSFORMED FEATURE SPACE\n",
    "# Description: Apply log1p to skewed numeric features before standardization\n",
    "# Purpose: Reduce impact of extreme values, improve linear model assumptions\n",
    "# ==============================================================================\n",
    "\n",
    "# Create a copy of X and apply log transform to skewed features\n",
    "X_z1 = X.copy()\n",
    "\n",
    "# Apply log1p to skewed features that exist in the dataset\n",
    "skewed_in_data = [col for col in SKEWED_FEATURES if col in X_z1.columns]\n",
    "for col in skewed_in_data:\n",
    "    # Clip to ensure non-negative values before log transform\n",
    "    X_z1[col] = np.log1p(X_z1[col].clip(lower=0))\n",
    "\n",
    "print(f\"Z1 (Log-transformed) feature space created:\")\n",
    "print(f\"  Skewed features transformed: {len(skewed_in_data)}\")\n",
    "print(f\"  Features: {skewed_in_data[:5]}... (showing first 5)\")\n",
    "\n",
    "# Use same preprocessor as Z0 (will standardize the log-transformed features)\n",
    "preprocessor_z1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"  Preprocessor: Same as Z0 (median impute + scale)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb797730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z2 (Polynomial) feature space definition:\n",
      "  Base features for polynomial expansion: ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath']\n",
      "  Polynomial degree: 2\n",
      "  Note: Will be combined with Z0 baseline features\n",
      "  Implementation: PolynomialFeatures(degree=2, include_bias=False)\n",
      "  Combined with baseline Z0 preprocessing\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Z2: POLYNOMIAL FEATURE SPACE\n",
    "# Description: Add degree-2 polynomial features for key predictors\n",
    "# Purpose: Capture non-linear relationships and feature interactions\n",
    "# Warning: Increases dimensionality significantly, use with regularization\n",
    "# ==============================================================================\n",
    "\n",
    "# Select key features for polynomial expansion (from EDA correlation analysis)\n",
    "poly_features = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath']\n",
    "poly_features = [f for f in poly_features if f in numeric_features]\n",
    "\n",
    "print(f\"Z2 (Polynomial) feature space definition:\")\n",
    "print(f\"  Base features for polynomial expansion: {poly_features}\")\n",
    "print(f\"  Polynomial degree: 2\")\n",
    "print(f\"  Note: Will be combined with Z0 baseline features\")\n",
    "\n",
    "# Preprocessor for Z2: Apply polynomial features to selected numerics, then standardize\n",
    "# We'll create this as a pipeline that first extracts poly features, then applies Z0\n",
    "\n",
    "# Note: Full implementation will be done when training models\n",
    "# For now, we define the feature names and strategy\n",
    "print(\"  Implementation: PolynomialFeatures(degree=2, include_bias=False)\")\n",
    "print(\"  Combined with baseline Z0 preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e0f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 (PCA-compressed) feature space definition:\n",
      "  Target components: 11 (explains ~90.0% variance)\n",
      "  Applied to: 36 numeric features\n",
      "  Categorical features: Kept as one-hot encoded (not compressed)\n",
      "  Preprocessor created: PCA(11) + StandardScaler + OneHotEncoder\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Z3: PCA-COMPRESSED FEATURE SPACE\n",
    "# Description: Apply PCA to numeric features to reduce dimensionality\n",
    "# Purpose: Reduce multicollinearity, speed up training, compress redundant info\n",
    "# Based on: PCA analysis showing 90% variance captured by ~11 components\n",
    "# ==============================================================================\n",
    "\n",
    "# Configuration for PCA\n",
    "PCA_N_COMPONENTS = 11  # Captures ~90% variance based on EDA analysis\n",
    "PCA_VARIANCE_THRESHOLD = 0.90\n",
    "\n",
    "print(f\"Z3 (PCA-compressed) feature space definition:\")\n",
    "print(f\"  Target components: {PCA_N_COMPONENTS} (explains ~{PCA_VARIANCE_THRESHOLD*100}% variance)\")\n",
    "print(f\"  Applied to: {len(numeric_features)} numeric features\")\n",
    "print(f\"  Categorical features: Kept as one-hot encoded (not compressed)\")\n",
    "\n",
    "# Create PCA transformer for numeric features\n",
    "numeric_transformer_pca = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=PCA_N_COMPONENTS, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Preprocessor for Z3: PCA on numerics, one-hot on categoricals\n",
    "preprocessor_z3 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_pca', numeric_transformer_pca, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(f\"  Preprocessor created: PCA({PCA_N_COMPONENTS}) + StandardScaler + OneHotEncoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e880e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE SPACE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Z0: Baseline\n",
      "  Standard preprocessing (median impute + scale + one-hot)\n",
      "  Preprocessor: Ready\n",
      "\n",
      "Z1: Log-transformed\n",
      "  Log1p on 18 skewed features + standard preprocessing\n",
      "  Preprocessor: Ready\n",
      "\n",
      "Z2: Polynomial\n",
      "  Degree-2 polynomials on 5 key features + baseline\n",
      "  Preprocessor: Will be built during model training\n",
      "\n",
      "Z3: PCA-compressed\n",
      "  PCA(11 components) on numerics + one-hot categoricals\n",
      "  Preprocessor: Ready\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary: All feature spaces defined\n",
    "feature_spaces = {\n",
    "    'Z0': {\n",
    "        'name': 'Baseline',\n",
    "        'description': 'Standard preprocessing (median impute + scale + one-hot)',\n",
    "        'preprocessor': preprocessor_z0,\n",
    "        'X_data': X\n",
    "    },\n",
    "    'Z1': {\n",
    "        'name': 'Log-transformed',\n",
    "        'description': f'Log1p on {len(skewed_in_data)} skewed features + standard preprocessing',\n",
    "        'preprocessor': preprocessor_z1,\n",
    "        'X_data': X_z1\n",
    "    },\n",
    "    'Z2': {\n",
    "        'name': 'Polynomial',\n",
    "        'description': f'Degree-2 polynomials on {len(poly_features)} key features + baseline',\n",
    "        'preprocessor': None,  # Will be created during model training\n",
    "        'X_data': X\n",
    "    },\n",
    "    'Z3': {\n",
    "        'name': 'PCA-compressed',\n",
    "        'description': f'PCA({PCA_N_COMPONENTS} components) on numerics + one-hot categoricals',\n",
    "        'preprocessor': preprocessor_z3,\n",
    "        'X_data': X\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE SPACE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for key, space in feature_spaces.items():\n",
    "    print(f\"\\n{key}: {space['name']}\")\n",
    "    print(f\"  {space['description']}\")\n",
    "    if space['preprocessor'] is not None:\n",
    "        print(f\"  Preprocessor: Ready\")\n",
    "    else:\n",
    "        print(f\"  Preprocessor: Will be built during model training\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
